import os
import torch
import gc
from nlp2 import set_seed
from LLAMA_Model import LLAMASeq2Seq
import sys
from transformers import AutoTokenizer, AutoModelForCausalLM

# 设置随机种子
set_seed(42)

# 获取 CSV 文件名作为命令行参数
input_folder = 'dataset/repaired/'
output_folder = 'save_model/'
filename = sys.argv[1]

# 构造路径
train_file_path = os.path.join(input_folder, filename)
model_name = filename.split('.')[0]
model_output_dir = os.path.join(output_folder, model_name)

# 创建输出目录
if not os.path.exists(model_output_dir):
    os.makedirs(model_output_dir)

# 释放内存
torch.cuda.empty_cache()
gc.collect()

# 创建并训练模型
model = LLAMASeq2Seq(base_model_path="../CodeLlama-7b-Python-hf/", load_path="None", add_eos_token=False, source_len=256, cutoff_len=512)

model.train(train_filename=train_file_path, train_batch_size=1, learning_rate=5e-5, num_train_epochs=5, output_dir=model_output_dir)

print(f"Model for {filename} saved to {model_output_dir}")

# 完成后，清理并释放内存
del model
torch.cuda.empty_cache()
gc.collect()
